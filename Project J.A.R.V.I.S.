import os
import json
import logging
import torch
import time
import re
import asyncio
from collections import deque
from cryptography.fernet import Fernet
from pathlib import Path
from transformers import pipeline
from whisper import load_model
from typing import Dict, Optional, Callable
from dataclasses import dataclass, field
from logging.handlers import RotatingFileHandler

# IMPORTANT:
# • Ensure this file has a .py extension.
# • Set up a pyproject.toml file for project configuration.
# • Consider splitting your code into multiple modules.
# • Consider extracting large data structures (such as model configurations) into external files.
# • All comments and messages in this file are in English.

PROJECT_NAME = "Project J.AR.V.I.S."

# SECRET_KEY is read from the environment variable; if not defined, an error is raised.
SECRET_KEY = os.getenv('AI_SECRET_KEY')
if not SECRET_KEY:
    raise ValueError("AI_SECRET_KEY environment variable is not defined!")
SECRET_KEY = SECRET_KEY.encode()

# Model Structure
@dataclass
class AIModel:
    name: str
    type: str
    model: Optional[object] = None
    processor: Optional[Callable] = None
    config: Dict = field(default_factory=dict)
    active: bool = False          # Manually set active/passive state
    always_active: bool = False   # Models that must always be active
    thinking_enabled: bool = False  # Enable thinking mode before generating response
    thinking_delay: float = 2.0     # Thinking delay in seconds

# Main AI System
class LocalAISystem:
    def __init__(self, quantization_bits: int = 8, temperature: float = 0.7):
        """
        :param quantization_bits: 8 or 4 (if CUDA is supported, the corresponding quantization is applied)
        :param temperature: Temperature setting used for model's response generation.
        """
        self.logger = self._setup_logging()
        self.logger.info(f"{PROJECT_NAME} started.")
        self.quantization_bits = quantization_bits
        self.temperature = temperature
        self.models = self._initialize_models()

        # Assign default processor to models without one assigned
        for model in self.models.values():
            if model.processor is None:
                model.processor = self._default_processor
        self.dialogue_history = deque(maxlen=20)  # Store last 20 conversations
        self.user_preferences = self._load_preferences()  # Encrypted preferences

        # Load dynamic banned_words and censorship_level from preferences
        self.banned_words = self.user_preferences.get("banned_words", ["badword", "worseword"])
        self.censorship_level = self.user_preferences.get("censorship_level", 0)

        # Set up command handlers for a better pattern than cascading if statements
        self.command_handlers = {
            "activate model": self._handle_activate_model,
            "deactivate model": self._handle_deactivate_model,
            "set temperature": self._handle_set_temperature,
            "set thinking_delay": self._handle_set_thinking_delay,
            "toggle thinking": self._handle_toggle_thinking,
            "set censorship": self._handle_set_censorship,
        }

    def _setup_logging(self) -> logging.Logger:
        logger = logging.getLogger("LocalAISystem")
        handler = RotatingFileHandler("ai.log", maxBytes=1e6, backupCount=3)
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)
        return logger

    def _default_processor(self, model: AIModel, input_data):
        raise NotImplementedError(f"No processor defined for {model.name}!")

    def _initialize_models(self) -> Dict[str, AIModel]:
        # For text models, thinking_delay = 30 seconds; for audio models, 20 seconds
        return {
            "mistral_7b": AIModel(
                name="Mistral 7B",
                type="text",
                config={
                    "model_name": "mistral-community/Mistral-7B-v0.1",
                    "task": "text-generation",
                    "max_new_tokens": 150
                },
                always_active=True,
                active=True,
                thinking_enabled=True,
                thinking_delay=30.0
            ),
            "deepseek": AIModel(
                name="Medical Insight AI",
                type="text",
                config={
                    "model_name": "gpt2",
                    "task": "text-generation",
                    "max_new_tokens": 150
                },
                always_active=False,
                active=False,
                thinking_delay=30.0
            ),
            "openassistant": AIModel(
                name="Financial Analysis AI",
                type="text",
                config={
                    "model_name": "gpt2",
                    "task": "text-generation",
                    "max_new_tokens": 150
                },
                always_active=False,
                active=False,
                thinking_delay=30.0
            ),
            "zephyr": AIModel(
                name="Legal Advisor AI",
                type="text",
                config={
                    "model_name": "gpt2",
                    "task": "text-generation",
                    "max_new_tokens": 150
                },
                always_active=False,
                active=False,
                thinking_delay=30.0
            ),
            "starcoder": AIModel(
                name="Scientific Research AI",
                type="text",
                config={
                    "model_name": "bigcode/starcoder",
                    "task": "text-generation",
                    "max_new_tokens": 150
                },
                always_active=False,
                active=False,
                thinking_delay=30.0
            ),
            "llama_3": AIModel(
                name="Creative Writing AI",
                type="text",
                config={
                    "model_name": "gpt2",
                    "task": "text-generation",
                    "max_new_tokens": 150
                },
                always_active=True,
                active=True,
                thinking_enabled=True,
                thinking_delay=30.0
            ),
            "whisper": AIModel(
                name="Speech Emotion Recognition AI",
                type="audio",
                config={
                    "model_size": "small"
                },
                always_active=False,
                active=False,
                thinking_delay=20.0
            ),
            "sentiment": AIModel(
                name="Social Media Analysis AI",
                type="text",
                config={
                    "model_name": "distilbert-base-uncased-finetuned-sst-2-english",
                    "task": "sentiment-analysis"
                },
                always_active=True,
                active=True,
                thinking_delay=30.0
            ),
            "weather_ai": AIModel(
                name="Weather Forecasting AI",
                type="text",
                config={
                    "model_name": "gpt2",
                    "task": "text-generation",
                    "max_new_tokens": 150
                },
                always_active=False,
                active=False,
                thinking_delay=30.0
            ),
            "travel_ai": AIModel(
                name="Travel Recommendation AI",
                type="text",
                config={
                    "model_name": "gpt2",
                    "task": "text-generation",
                    "max_new_tokens": 150
                },
                always_active=False,
                active=False,
                thinking_delay=30.0
            ),
            "education_ai": AIModel(
                name="Educational Tutor AI",
                type="text",
                config={
                    "model_name": "gpt2",
                    "task": "text-generation",
                    "max_new_tokens": 150
                },
                always_active=False,
                active=False,
                thinking_delay=30.0
            ),
            "entertainment_ai": AIModel(
                name="Entertainment Suggestion AI",
                type="text",
                config={
                    "model_name": "gpt2",
                    "task": "text-generation",
                    "max_new_tokens": 150
                },
                always_active=False,
                active=False,
                thinking_delay=30.0
            ),
            "sports_ai": AIModel(
                name="Sports Analysis AI",
                type="text",
                config={
                    "model_name": "gpt2",
                    "task": "text-generation",
                    "max_new_tokens": 150
                },
                always_active=False,
                active=False,
                thinking_delay=30.0
            ),
            "agriculture_ai": AIModel(
                name="Agriculture Advisor AI",
                type="text",
                config={
                    "model_name": "gpt2",
                    "task": "text-generation",
                    "max_new_tokens": 150
                },
                always_active=False,
                active=False,
                thinking_delay=30.0
            ),
            "cybersec_ai": AIModel(
                name="Cybersecurity AI",
                type="text",
                config={
                    "model_name": "gpt2",
                    "task": "text-generation",
                    "max_new_tokens": 150
                },
                always_active=False,
                active=False,
                thinking_delay=30.0
            ),
            "logistics_ai": AIModel(
                name="Logistics Optimization AI",
                type="text",
                config={
                    "model_name": "gpt2",
                    "task": "text-generation",
                    "max_new_tokens": 150
                },
                always_active=False,
                active=False,
                thinking_delay=30.0
            ),
            "energy_ai": AIModel(
                name="Energy Consumption AI",
                type="text",
                config={
                    "model_name": "gpt2",
                    "task": "text-generation",
                    "max_new_tokens": 150
                },
                always_active=False,
                active=False,
                thinking_delay=30.0
            ),
            "ecommerce_ai": AIModel(
                name="E-commerce Trend AI",
                type="text",
                config={
                    "model_name": "gpt2",
                    "task": "text-generation",
                    "max_new_tokens": 150
                },
                always_active=False,
                active=False,
                thinking_delay=30.0
            )
        }

    def load_models(self):
        if torch.cuda.is_available():
            dtype = torch.float16
            device_map = "auto"
            if self.quantization_bits == 8:
                quantization_config = {"load_in_8bit": True}
            elif self.quantization_bits == 4:
                quantization_config = {"load_in_4bit": True}
            else:
                quantization_config = {}
        elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            dtype = None
            device_map = "auto"
            quantization_config = {}
        else:
            dtype = None
            device_map = None
            quantization_config = {}

        for model_id, model in self.models.items():
            try:
                if model.type == "audio":
                    model.model = load_model(model.config["model_size"])
                    model.processor = self._process_audio
                else:
                    task = model.config.get("task", "text-generation")
                    model.model = pipeline(
                        task,
                        model=model.config["model_name"],
                        device_map=device_map,
                        torch_dtype=dtype,
                        max_new_tokens=model.config.get("max_new_tokens", 150),
                        **quantization_config
                    )
                    if task == "sentiment-analysis":
                        model.processor = self._process_sentiment
                    else:
                        model.processor = self._process_text

                if not model.model:
                    self.logger.error(f"{model.name} model could not be loaded correctly. This model will not be used.")
                    model.active = False
                    model.always_active = False
                    continue

                self.logger.info(f"Model loaded: {model.name}")
            except Exception as e:
                self.logger.error(f"{model.name} could not be loaded: {str(e)}")
                model.active = False
                model.always_active = False

    def _load_preferences(self, filename="preferences.json") -> Dict:
        file_path = Path(filename)
        if not file_path.is_file():
            self.logger.warning(f"{filename} not found!")
            return {
                "frequently_used": [],
                "personal_info": {},
                "data_saving_enabled": True,
                "custom_data": {},
                "reasoning_enabled": True,
                "banned_words": ["badword", "worseword"],
                "censorship_level": 0
            }
        try:
            with file_path.open("rb") as f:
                encrypted_data = f.read()
            decrypted = Fernet(SECRET_KEY).decrypt(encrypted_data)
            return json.loads(decrypted.decode())
        except (json.JSONDecodeError, Exception) as e:
            self.logger.error(f"Preference file has invalid JSON format or could not be decrypted: {str(e)}")
            return {
                "frequently_used": [],
                "personal_info": {},
                "data_saving_enabled": True,
                "custom_data": {},
                "reasoning_enabled": True,
                "banned_words": ["badword", "worseword"],
                "censorship_level": 0
            }

    def _save_preferences(self, filename="preferences.json"):
        encrypted = Fernet(SECRET_KEY).encrypt(
            json.dumps(self.user_preferences).encode()
        )
        with open(filename, "wb") as f:
            f.write(encrypted)

    def remember(self, text: str) -> str:
        text = text[len("remember that"):].strip()
        if ' is ' not in text:
            return "Incorrect format. Please use 'remember that [key] is [value]'."
        key, value = text.split(' is ', 1)
        self.user_preferences["custom_data"][key.strip()] = value.strip()
        self._save_preferences()
        return f"'{key.strip()}' saved: {value.strip()}"

    def save_history(self, filename="history.json"):
        with open(filename, "w") as f:
            json.dump(list(self.dialogue_history), f)

    def load_history(self, filename="history.json"):
        try:
            with open(filename, "r") as f:
                self.dialogue_history = deque(json.load(f), maxlen=20)
        except FileNotFoundError:
            self.logger.warning("History file not found, starting with empty history.")
            self.dialogue_history = deque(maxlen=20)

    def apply_censorship(self, text: str) -> str:
        if self.censorship_level <= 0:
            return text
        censored_text = text
        for word in self.banned_words:
            replacement = "*" * len(word)
            censored_text = re.sub(re.escape(word), replacement, censored_text, flags=re.IGNORECASE)
        return censored_text

    def process_input(self, input_text: str) -> str:
        if not input_text.strip():
            return "Empty input is not allowed."
        original_input = input_text.strip()
        lower_input = original_input.lower()
        try:
            if lower_input.startswith("remember that"):
                return self.remember(original_input)
            elif original_input.endswith((".wav", ".mp3")):
                return self.models["whisper"].processor(self.models["whisper"], original_input)
            elif lower_input.startswith("sentiment:"):
                text = original_input[len("sentiment:"):].strip()
                return self.models["sentiment"].processor(self.models["sentiment"], text)
            else:
                # Use command handlers for a better pattern than cascading if statements
                for command, handler in self.command_handlers.items():
                    if lower_input.startswith(command):
                        return handler(original_input)
                # Default: aggregate responses from active text models
                aggregated = ""
                for model_id, model in self.models.items():
                    if model.type != "audio" and (model.always_active or model.active):
                        try:
                            resp = model.processor(model, original_input)
                            aggregated += f"{model.name} response: {resp}\n"
                        except Exception as e:
                            self.logger.exception(f"{model.name} processing error:")
                            aggregated += f"{model.name} response: An error occurred.\n"
                unified_prompt = (
                    "Synthesize a single, coherent, and understandable answer from the responses of the following AI systems:\n\n"
                    + aggregated +
                    "\nPlease provide a single answer."
                )
                unified_response = self.models["mistral_7b"].processor(self.models["mistral_7b"], unified_prompt)
                unified_response = self.apply_censorship(unified_response)
                self.dialogue_history.append(f"Monica synthesis prompt: {unified_prompt}")
                self.dialogue_history.append(f"Monica response: {unified_response}")
                self.save_history()
                return unified_response
        except Exception as e:
            self.logger.exception("A critical error occurred:")
            return f"System error: {str(e)} - Please contact the administrator."

    def _route_to_model(self, model_key: str, prompt: str) -> str:
        if model_key in self.models:
            response = self.models[model_key].processor(self.models[model_key], prompt)
            self.dialogue_history.append(prompt)
            self.dialogue_history.append(response)
            self.save_history()
            return response
        return "Model not found."

    def _process_text(self, model: AIModel, input_text: str) -> str:
        if model.thinking_enabled:
            self.logger.info(f"{model.name} - Thinking... This will take {model.thinking_delay} seconds.")
            asyncio.run(asyncio.sleep(model.thinking_delay))
            self.logger.info(f"{model.name} - Preparing response...")
        try:
            with torch.inference_mode():
                generated = model.model(
                    input_text,
                    max_new_tokens=model.config.get("max_new_tokens", 150),
                    do_sample=True,
                    temperature=self.temperature,
                )
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            if generated and isinstance(generated, list) and 'generated_text' in generated[0]:
                return generated[0]['generated_text']
            else:
                self.logger.error(f"{model.name} returned unexpected format: {generated}")
                return "Response could not be generated."
        except Exception as e:
            self.logger.exception("Text processing error:")
            return "An error occurred."

    def _process_audio(self, model: AIModel, file_path: str) -> str:
        if not Path(file_path).exists():
            return "File not found."
        try:
            result = model.model.transcribe(file_path)
            return result["text"]
        except Exception as e:
            self.logger.exception("Audio processing error:")
            return "Audio file could not be processed."

    def _process_sentiment(self, model: AIModel, text: str) -> str:
        try:
            result = model.model(text)
            if result and isinstance(result, list) and 'label' in result[0]:
                return f"Sentiment: {result[0]['label']} (Score: {result[0]['score']:.2f})"
            else:
                self.logger.error(f"{model.name} returned unexpected format: {result}")
                return "Sentiment analysis failed."
        except Exception as e:
            self.logger.exception("Sentiment analysis error:")
            return "Sentiment analysis failed."

    # Command handler methods for refactored command pattern
    def _handle_activate_model(self, input_text: str) -> str:
        parts = input_text.split()
        if len(parts) >= 3:
            model_key = parts[2]
            if model_key in self.models:
                if self.models[model_key].always_active:
                    return f"{self.models[model_key].name} is already always active."
                self.models[model_key].active = True
                return f"{self.models[model_key].name} has been activated."
            else:
                return "Model not found."
        else:
            return "Please enter 'activate model [model_name]'."

    def _handle_deactivate_model(self, input_text: str) -> str:
        parts = input_text.split()
        if len(parts) >= 3:
            model_key = parts[2]
            if model_key in self.models:
                if self.models[model_key].always_active:
                    return f"{self.models[model_key].name} cannot be deactivated because it is always active."
                self.models[model_key].active = False
                return f"{self.models[model_key].name} has been deactivated."
            else:
                return "Model not found."
        else:
            return "Please enter 'deactivate model [model_name]'."

    def _handle_set_temperature(self, input_text: str) -> str:
        parts = input_text.split()
        try:
            new_temp = float(parts[-1])
            self.temperature = new_temp
            return f"Temperature setting updated to {new_temp}."
        except ValueError:
            return "Please enter a valid number."

    def _handle_set_thinking_delay(self, input_text: str) -> str:
        parts = input_text.split()
        if len(parts) >= 4:
            model_key = parts[2]
            try:
                new_delay = float(parts[3])
            except ValueError:
                return "Please enter a valid number."
            if new_delay < 0 or new_delay > 10:
                return "Thinking delay must be between 0 and 10 seconds."
            if model_key in self.models:
                self.models[model_key].thinking_delay = new_delay
                return f"Thinking delay for {self.models[model_key].name} updated to {new_delay} seconds."
            else:
                return "Model not found."
        else:
            return "Please enter 'set thinking_delay [model_name] [value]'."

    def _handle_toggle_thinking(self, input_text: str) -> str:
        parts = input_text.split()
        if len(parts) >= 3:
            model_key = parts[2]
            if model_key in self.models:
                self.models[model_key].thinking_enabled = not self.models[model_key].thinking_enabled
                status = "enabled" if self.models[model_key].thinking_enabled else "disabled"
                return f"Thinking mode for {self.models[model_key].name} has been {status}."
            else:
                return "Model not found."
        else:
            return "Please enter 'toggle thinking [model_name]'."

    def _handle_set_censorship(self, input_text: str) -> str:
        parts = input_text.split()
        if len(parts) >= 3:
            try:
                new_censorship = int(parts[2])
            except ValueError:
                return "Please enter a valid number."
            self.censorship_level = new_censorship
            return f"Censorship level updated to {new_censorship}."
        else:
            return "Please enter 'set censorship [value]'."

# Main program
def main():
    # Ensure the file has a .py extension and set up a pyproject.toml for project configuration.
    # Consider splitting the code into multiple modules and extracting large data structures into external files.
    system = LocalAISystem(quantization_bits=8, temperature=0.7)
    system.load_models()
    system.load_history()

    print(f"{PROJECT_NAME} - Commands:")
    print("- Audio file: 'file.wav' or 'file.mp3'")
    print("- Sentiment analysis: 'sentiment: <text>'")
    print("- Memory record: 'remember that [info] is [value]'")
    print("- Activate model: 'activate model [model_name]'")
    print("- Deactivate model: 'deactivate model [model_name]'")
    print("- Set temperature: 'set temperature [value]'")
    print("- Set thinking delay: 'set thinking_delay [model_name] [value]'")
    print("- Toggle thinking: 'toggle thinking [model_name]'")
    print("- Set censorship: 'set censorship [value]'")
    print("- All other text queries will be automatically answered with a synthesized response in the style of Monica.")
    print("- Exit: 'exit'")

    while True:
        user_input = input(">> ")
        if user_input.lower() == "exit":
            system.save_history()
            break
        result = system.process_input(user_input)
        print(f"\nResponse: {result}\n")

if __name__ == "__main__":
    main()
