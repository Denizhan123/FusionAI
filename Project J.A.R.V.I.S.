import os
import json
import logging
import torch
import time
import re
import asyncio
from collections import deque
from cryptography.fernet import Fernet
from pathlib import Path
from transformers import pipeline
from whisper import load_model
from typing import Dict, Optional, Callable
from dataclasses import dataclass, field
from logging.handlers import RotatingFileHandler

# Sabitler
PROJECT_NAME = "Project J.AR.V.I.S."
PREFERENCES_FILE = "preferences.json"
HISTORY_FILE = "history.json"
MAX_HISTORY_LENGTH = 20
LOG_FILE_SIZE = 1e6
LOG_BACKUP_COUNT = 3
DEFAULT_THINKING_DELAY = 2.0
MAX_THINKING_DELAY = 10.0

# SECRET_KEY, ortam değişkeninden okunuyor; tanımlı değilse hata veriliyor.
SECRET_KEY = os.getenv('AI_SECRET_KEY')
if not SECRET_KEY:
    raise ValueError("AI_SECRET_KEY ortam değişkeni tanımlı değil!")
SECRET_KEY = SECRET_KEY.encode()

# Model Yapısı
@dataclass
class AIModel:
    name: str
    type: str
    model: Optional[object] = None
    processor: Optional[Callable] = None
    config: Dict = field(default_factory=dict)
    active: bool = False          # Manuel olarak aktif/pasif durumu
    always_active: bool = False   # Her zaman aktif olması gereken modeller
    thinking_enabled: bool = False  # Cevap üretmeden önce düşünme modu
    thinking_delay: float = DEFAULT_THINKING_DELAY     # Düşünme gecikmesi (saniye cinsinden)

# Ana Yapay Zeka Sistemi
class LocalAISystem:
    def __init__(self, quantization_bits: int = 8, temperature: float = 0.7):
        """
        :param quantization_bits: 8 veya 4 (CUDA destekliyorsa ilgili quantization uygulanır)
        :param temperature: Modelin yanıt üretiminde kullanılacak sıcaklık değeri.
        """
        self.logger = self._setup_logging()
        self.logger.info(f"{PROJECT_NAME} başlatıldı.")
        self.quantization_bits = quantization_bits
        self.temperature = temperature
        self.models = self._initialize_models()
        # Processor atanması yapılmamış modellere varsayılan işlemci ataması
        for model in self.models.values():
            if model.processor is None:
                model.processor = self._default_processor
        self.dialogue_history = deque(maxlen=MAX_HISTORY_LENGTH)  # Son 20 konuşmayı sakla
        self.user_preferences = self._load_preferences()  # Şifreli tercihler
        # Preferences içinden dinamik banned_words ve censorship_level yükleniyor
        self.banned_words = self.user_preferences.get("banned_words", ["badword", "worseword"])
        self.censorship_level = self.user_preferences.get("censorship_level", 0)
        self.load_models()  # Modelleri yükle
        # Temel modelin (mistral_7b) yüklendiğinden emin olun
        if "mistral_7b" not in self.models or not self.models["mistral_7b"].model:
            raise RuntimeError("Temel model (mistral_7b) yüklenemedi! Sistem başlatılamıyor.")

    # Loglama Ayarları
    def _setup_logging(self) -> logging.Logger:
        logger = logging.getLogger("LocalAISystem")
        handler = RotatingFileHandler("ai.log", maxBytes=LOG_FILE_SIZE, backupCount=LOG_BACKUP_COUNT)
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)
        return logger

    # Varsayılan İşlemci
    def _default_processor(self, model: AIModel, input_data):
        raise NotImplementedError(f"{model.name} için işlemci tanımlı değil!")

    # Modelleri Başlatma
    def _initialize_models(self) -> Dict[str, AIModel]:
        # Metin modelleri için thinking_delay = 30 saniye, audio modelleri için thinking_delay = 20 saniye
        return {
            "mistral_7b": AIModel(
                name="Mistral 7B",
                type="text",
                config={
                    "model_name": "mistral-community/Mistral-7B-v0.1",
                    "task": "text-generation",
                    "max_new_tokens": 150
                },
                always_active=True,
                active=True,
                thinking_enabled=True,
                thinking_delay=30.0
            ),
            "deepseek": AIModel(
                name="Medical Insight AI",
                type="text",
                config={
                    "model_name": "gpt2",
                    "task": "text-generation",
                    "max_new_tokens": 150
                },
                always_active=False,
                active=False,
                thinking_delay=30.0
            ),
            "openassistant": AIModel(
                name="Financial Analysis AI",
                type="text",
                config={
                    "model_name": "gpt2",
                    "task": "text-generation",
                    "max_new_tokens": 150
                },
                always_active=False,
                active=False,
                thinking_delay=30.0
            ),
            "zephyr": AIModel(
                name="Legal Advisor AI",
                type="text",
                config={
                    "model_name": "gpt2",
                    "task": "text-generation",
                    "max_new_tokens": 150
                },
                always_active=False,
                active=False,
                thinking_delay=30.0
            ),
            "starcoder": AIModel(
                name="Scientific Research AI",
                type="text",
                config={
                    "model_name": "bigcode/starcoder",
                    "task": "text-generation",
                    "max_new_tokens": 150
                },
                always_active=False,
                active=False,
                thinking_delay=30.0
            ),
            "llama_3": AIModel(
                name="Creative Writing AI",
                type="text",
                config={
                    "model_name": "gpt2",
                    "task": "text-generation",
                    "max_new_tokens": 150
                },
                always_active=True,
                active=True,
                thinking_enabled=True,
                thinking_delay=30.0
            ),
            "whisper": AIModel(
                name="Speech Emotion Recognition AI",
                type="audio",
                config={
                    "model_size": "small"
                },
                always_active=False,
                active=False,
                thinking_delay=20.0
            ),
            "sentiment": AIModel(
                name="Social Media Analysis AI",
                type="text",
                config={
                    "model_name": "distilbert-base-uncased-finetuned-sst-2-english",
                    "task": "sentiment-analysis"
                },
                always_active=True,
                active=True,
                thinking_delay=30.0
            ),
            "weather_ai": AIModel(
                name="Weather Forecasting AI",
                type="text",
                config={
                    "model_name": "gpt2",
                    "task": "text-generation",
                    "max_new_tokens": 150
                },
                always_active=False,
                active=False,
                thinking_delay=30.0
            ),
            "travel_ai": AIModel(
                name="Travel Recommendation AI",
                type="text",
                config={
                    "model_name": "gpt2",
                    "task": "text-generation",
                    "max_new_tokens": 150
                },
                always_active=False,
                active=False,
                thinking_delay=30.0
            ),
            "education_ai": AIModel(
                name="Educational Tutor AI",
                type="text",
                config={
                    "model_name": "gpt2",
                    "task": "text-generation",
                    "max_new_tokens": 150
                },
                always_active=False,
                active=False,
                thinking_delay=30.0
            ),
            "entertainment_ai": AIModel(
                name="Entertainment Suggestion AI",
                type="text",
                config={
                    "model_name": "gpt2",
                    "task": "text-generation",
                    "max_new_tokens": 150
                },
                always_active=False,
                active=False,
                thinking_delay=30.0
            ),
            "sports_ai": AIModel(
                name="Sports Analysis AI",
                type="text",
                config={
                    "model_name": "gpt2",
                    "task": "text-generation",
                    "max_new_tokens": 150
                },
                always_active=False,
                active=False,
                thinking_delay=30.0
            ),
            "agriculture_ai": AIModel(
                name="Agriculture Advisor AI",
                type="text",
                config={
                    "model_name": "gpt2",
                    "task": "text-generation",
                    "max_new_tokens": 150
                },
                always_active=False,
                active=False,
                thinking_delay=30.0
            ),
            "cybersec_ai": AIModel(
                name="Cybersecurity AI",
                type="text",
                config={
                    "model_name": "gpt2",
                    "task": "text-generation",
                    "max_new_tokens": 150
                },
                always_active=False,
                active=False,
                thinking_delay=30.0
            ),
            "logistics_ai": AIModel(
                name="Logistics Optimization AI",
                type="text",
                config={
                    "model_name": "gpt2",
                    "task": "text-generation",
                    "max_new_tokens": 150
                },
                always_active=False,
                active=False,
                thinking_delay=30.0
            ),
            "energy_ai": AIModel(
                name="Energy Consumption AI",
                type="text",
                config={
                    "model_name": "gpt2",
                    "task": "text-generation",
                    "max_new_tokens": 150
                },
                always_active=False,
                active=False,
                thinking_delay=30.0
            ),
            "ecommerce_ai": AIModel(
                name="E-commerce Trend AI",
                type="text",
                config={
                    "model_name": "gpt2",
                    "task": "text-generation",
                    "max_new_tokens": 150
                },
                always_active=False,
                active=False,
                thinking_delay=30.0
            )
        }

    # Modelleri Yükleme
    def load_models(self):
        if torch.cuda.is_available():
            dtype = torch.float16
            device_map = "auto"
            if self.quantization_bits == 8:
                quantization_config = {"load_in_8bit": True}
            elif self.quantization_bits == 4:
                quantization_config = {"load_in_4bit": True}
            else:
                quantization_config = {}
        elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            dtype = None
            device_map = "auto"
            quantization_config = {}
        else:
            dtype = None
            device_map = None
            quantization_config = {}

        essential_model_loaded = False

        for model_id, model in self.models.items():
            try:
                if model.type == "audio":
                    model.model = load_model(model.config["model_size"])
                    model.processor = self._process_audio
                else:
                    task = model.config.get("task", "text-generation")
                    model.model = pipeline(
                        task,
                        model=model.config["model_name"],
                        device_map=device_map,
                        torch_dtype=dtype,
                        max_new_tokens=model.config.get("max_new_tokens", 150),
                        **quantization_config
                    )
                    if task == "sentiment-analysis":
                        model.processor = self._process_sentiment
                    else:
                        model.processor = self._process_text

                if not model.model:
                    self.logger.error(f"{model.name} modeli doğru yüklenemedi. Bu model kullanılmayacak.")
                    model.active = False
                    model.always_active = False
                    continue

                self.logger.info(f"Model yüklendi: {model.name}")
                if model_id == "mistral_7b":
                    essential_model_loaded = True
            except Exception as e:
                self.logger.exception(f"{model.name} yüklenemedi: {str(e)}")
                model.active = False
                model.always_active = False

        if not essential_model_loaded:
            raise RuntimeError("Temel model (mistral_7b) yüklenemedi! Sistem başlatılamıyor.")

    # Şifreli Tercih Yükleme
    def _load_preferences(self) -> Dict:
        file_path = Path(PREFERENCES_FILE)
        if not file_path.is_file():
            self.logger.warning(f"{PREFERENCES_FILE} bulunamadı!")
            return self._create_default_preferences()
        try:
            with file_path.open("rb") as f:
                encrypted_data = f.read()
                decrypted = Fernet(SECRET_KEY).decrypt(encrypted_data)
                return json.loads(decrypted.decode())
        except (json.JSONDecodeError, Exception) as e:
            self.logger.error(f"Tercih dosyası geçersiz JSON formatında veya deşifre edilemedi: {str(e)}")
            return self._create_default_preferences()

    def _create_default_preferences(self) -> Dict:
        default_prefs = {
            "frequently_used": [],
            "personal_info": {},
            "data_saving_enabled": True,
            "custom_data": {},
            "reasoning_enabled": True,
            "banned_words": ["badword", "worseword"],
            "censorship_level": 0
        }
        self.user_preferences = default_prefs
        return default_prefs

    # Şifreli Tercih Kaydetme
    def _save_preferences(self, filename=PREFERENCES_FILE):
        encrypted = Fernet(SECRET_KEY).encrypt(
            json.dumps(self.user_preferences).encode()
        )
        with open(filename, "wb") as f:
            f.write(encrypted)

    # Hafıza Yönetimi
    def remember(self, text: str) -> str:
        text = text[len("remember that"):].strip()
        if ' is ' not in text:
            return "Hatalı format. 'remember that [anahtar] is [değer]' şeklinde girin."
        key, value = text.split(' is ', 1)
        self.user_preferences["custom_data"][key.strip()] = value.strip()
        self._save_preferences()
        return f"'{key.strip()}' kaydedildi: {value.strip()}"

    # Konuşma Geçmişini Kaydetme
    def save_history(self, filename=HISTORY_FILE):
        with open(filename, "w") as f:
            json.dump(list(self.dialogue_history), f)

    # Konuşma Geçmişini Yükleme
    def load_history(self, filename=HISTORY_FILE):
        try:
            with open(filename, "r") as f:
                self.dialogue_history = deque(json.load(f), maxlen=MAX_HISTORY_LENGTH)
        except FileNotFoundError:
            self.logger.warning("History dosyası bulunamadı, boş bir geçmişle başlanıyor.")
            self.dialogue_history = deque(maxlen=MAX_HISTORY_LENGTH)

    # Sansür Uygulama Fonksiyonu
    def apply_censorship(self, text: str) -> str:
        if self.censorship_level <= 0:
            return text
        censored_text = text
        for word in self.banned_words:
            replacement = "*" * len(word)
            censored_text = re.sub(re.escape(word), replacement, censored_text, flags=re.IGNORECASE)
        return censored_text

    # Girdiyi İşleme
    def process_input(self, input_text: str) -> str:
        if not input_text.strip():
            return "Boş giriş yapılamaz."
        original_input = input_text.strip()
        lower_input = original_input.lower()
        try:
            if lower_input.startswith("remember that"):
                return self.remember(original_input)
            elif original_input.endswith((".wav", ".mp3")):
                return self.models["whisper"].processor(self.models["whisper"], original_input)
            elif lower_input.startswith("sentiment:"):
                text = original_input[len("sentiment:"):].strip()
                return self.models["sentiment"].processor(self.models["sentiment"], text)
            elif lower_input.startswith("activate model"):
                parts = original_input.split()
                if len(parts) >= 3:
                    model_key = parts[2]
                    if model_key in self.models:
                        if self.models[model_key].always_active:
                            return f"{self.models[model_key].name} zaten her zaman aktif."
                        self.models[model_key].active = True
                        return f"{self.models[model_key].name} aktive edildi."
                    else:
                        return "Model bulunamadı."
                else:
                    return "Lütfen 'activate model [model_adi]' şeklinde girin."
            elif lower_input.startswith("deactivate model"):
                parts = original_input.split()
                if len(parts) >= 3:
                    model_key = parts[2]
                    if model_key in self.models:
                        if self.models[model_key].always_active:
                            return f"{self.models[model_key].name} her zaman aktif olduğu için devre dışı bırakılamaz."
                        self.models[model_key].active = False
                        return f"{self.models[model_key].name} devre dışı bırakıldı."
                    else:
                        return "Model bulunamadı."
                else:
                    return "Lütfen 'deactivate model [model_adi]' şeklinde girin."
            elif lower_input.startswith("set temperature"):
                parts = original_input.split()
                try:
                    new_temp = float(parts[-1])
                    self.temperature = new_temp
                    return f"Temperature ayarı {new_temp} olarak güncellendi."
                except ValueError:
                    return "Geçerli bir sayı giriniz."
            elif lower_input.startswith("set thinking_delay"):
                parts = original_input.split()
                if len(parts) >= 4:
                    model_key = parts[2]
                    try:
                        new_delay = float(parts[3])
                    except ValueError:
                        return "Geçerli bir sayı giriniz."
                    if new_delay < 0 or new_delay > MAX_THINKING_DELAY:
                        return f"Thinking delay 0 ile {MAX_THINKING_DELAY} saniye arasında olmalıdır."
                    if model_key in self.models:
                        self.models[model_key].thinking_delay = new_delay
                        return f"{self.models[model_key].name} için thinking_delay ayarı {new_delay} saniye olarak güncellendi."
                    else:
                        return "Model bulunamadı."
                else:
                    return "Lütfen 'set thinking_delay [model_adi] [değer]' şeklinde girin."
            elif lower_input.startswith("toggle thinking"):
                parts = original_input.split()
                if len(parts) >= 3:
                    model_key = parts[2]
                    if model_key in self.models:
                        self.models[model_key].thinking_enabled = not self.models[model_key].thinking_enabled
                        status = "açık" if self.models[model_key].thinking_enabled else "kapalı"
                        return f"{self.models[model_key].name} için düşünme modu {status} hale getirildi."
                    else:
                        return "Model bulunamadı."
                else:
                    return "Lütfen 'toggle thinking [model_adi]' şeklinde girin."
            elif lower_input.startswith("set censorship"):
                parts = original_input.split()
                if len(parts) >= 3:
                    try:
                        new_censorship = int(parts[2])
                    except ValueError:
                        return "Geçerli bir sayı giriniz."
                    self.censorship_level = new_censorship
                    return f"Censorship seviyesi {new_censorship} olarak ayarlandı."
                else:
                    return "Lütfen 'set censorship [değer]' şeklinde girin."
            else:
                aggregated = ""
                for model_id, model in self.models.items():
                    if model.type != "audio" and (model.always_active or model.active):
                        try:
                            resp = model.processor(model, original_input)
                            aggregated += f"{model.name} yanıtı: {resp}\n"
                        except Exception as e:
                            self.logger.exception(f"{model.name} işleme hatası:")
                            aggregated += f"{model.name} yanıtı: Hata oluştu.\n"
                unified_prompt = (
                    "Aşağıdaki farklı yapay zeka sistemlerinden gelen bilgileri tek, tutarlı ve anlaşılır bir cevap "
                    "halinde sentezle:\n\n" + aggregated + "\nLütfen tek bir cevap ver."
                )
                unified_response = self.models["mistral_7b"].processor(self.models["mistral_7b"], unified_prompt)
                unified_response = self.apply_censorship(unified_response)
                self.dialogue_history.append(f"Monica sentezleme prompt: {unified_prompt}")
                self.dialogue_history.append(f"Monica yanıtı: {unified_response}")
                self.save_history()
                return unified_response
        except Exception as e:
            self.logger.exception("Kritik hata oluştu:")
            return f"Sistem hatası: {str(e)} - Lütfen yöneticiye bildirin."

    # Model Yönlendirme
    def _route_to_model(self, model_key: str, prompt: str) -> str:
        if model_key in self.models:
            response = self.models[model_key].processor(self.models[model_key], prompt)
            self.dialogue_history.append(prompt)
            self.dialogue_history.append(response)
            self.save_history()
            return response
        return "Model bulunamadı."

    # Metin İşleme
    def _process_text(self, model: AIModel, input_text: str) -> str:
        if model.thinking_enabled:
            self.logger.info(f"{model.name} - Düşünüyorum... Bu işlem {model.thinking_delay} saniye sürecek.")
            asyncio.run(asyncio.sleep(model.thinking_delay))
            self.logger.info(f"{model.name} - Cevap hazırlanıyor...")
        try:
            with torch.inference_mode():
                generated = model.model(
                    input_text,
                    max_new_tokens=model.config.get("max_new_tokens", 150),
                    do_sample=True,
                    temperature=self.temperature,
                )
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            if generated and isinstance(generated, list) and 'generated_text' in generated[0]:
                return generated[0]['generated_text']
            else:
                self.logger.error(f"{model.name} beklenmeyen çıktı formatı: {generated}")
                return "Cevap oluşturulamadı."
        except Exception as e:
            self.logger.exception("Metin işleme hatası:")
            return "Bir hata oluştu."

    # Ses İşleme
    def _process_audio(self, model: AIModel, file_path: str) -> str:
        if not Path(file_path).exists():
            return "Dosya bulunamadı."
        try:
            result = model.model.transcribe(file_path)
            return result["text"]
        except Exception as e:
            self.logger.exception("Ses işleme hatası:")
            return "Ses dosyası işlenemedi."

    # Duygu Analizi İşleme
    def _process_sentiment(self, model: AIModel, text: str) -> str:
        try:
            result = model.model(text)
            if result and isinstance(result, list) and 'label' in result[0]:
                return f"Duygu: {result[0]['label']} (Puan: {result[0]['score']:.2f})"
            else:
                self.logger.error(f"{model.name} beklenmeyen çıktı formatı: {result}")
                return "Duygu analizi başarısız."
        except Exception as e:
            self.logger.exception("Duygu analizi hatası:")
            return "Duygu analizi başarısız."

# Ana Program
def main():
    system = LocalAISystem(quantization_bits=8, temperature=0.7)
    system.load_history()

    print(f"{PROJECT_NAME} - Komutlar:")
    print("- Ses dosyası: 'file.wav' veya 'file.mp3'")
    print("- Duygu analizi: 'sentiment: <metin>'")
    print("- Bellek kaydı: 'remember that [bilgi] is [değer]'")
    print("- Model aktivasyonu: 'activate model [model_adi]'")
    print("- Model devre dışı bırakma: 'deactivate model [model_adi]'")
    print("- Sıcaklık ayarı: 'set temperature [değer]'")
    print("- Thinking delay ayarı: 'set thinking_delay [model_adi] [değer]'")
    print("- Toggle thinking: 'toggle thinking [model_adi]'")
    print("- Sansür ayarı: 'set censorship [değer]'")
    print("- Diğer tüm metin sorguları, otomatik olarak Monica tarzında sentezlenmiş cevapla yanıtlanır.")
    print("- Çıkış: 'exit'")

    while True:
        user_input = input(">> ")
        if user_input.lower() == "exit":
            system.save_history()
            break
        result = system.process_input(user_input)
        print(f"\nYanıt: {result}\n")

if __name__ == "__main__":
    main()
